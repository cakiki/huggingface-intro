{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77716564",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"text-align:center;\">ü¶ú Natural Language Processing with Transformers ü§ó</h1><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11d2af",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;\">Chapter 2. Text Classification</h2><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39cc27f",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center;\"><b>Christopher Akiki</b></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf53ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul>\n",
    "    <li><h3>The Dataset</h3></li>\n",
    "    <br>\n",
    "    <li><h3>From Text to Tokens</h3></li>\n",
    "    <br>\n",
    "    <li><h3>Training a Text Classifier</h3></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbb9897",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"images/chapter02_hf-libraries.png\" width=1800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6646d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"text-align:center;\">ü§ó Datasets</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0498b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Apache Arrow backend ‚û°Ô∏è Low RAM use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca1cc82",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration bigcode--the-stack-smol-2e98eace392455c7\n",
      "Found cached dataset json (/mnt/1da05489-3812-4f15-a6e5-c8d3c57df39e/cache/huggingface/bigcode___json/bigcode--the-stack-smol-2e98eace392455c7/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM usage when loading a 2.547GB dataset: 244.859375 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psutil\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "mem_before = psutil.Process(os.getpid()).memory_info().rss / (1024**2)\n",
    "\n",
    "stack_smol = load_dataset(\"bigcode/the-stack-smol\", split=\"train\")\n",
    "\n",
    "mem_after = psutil.Process(os.getpid()).memory_info().rss / (1024**2)\n",
    "\n",
    "print(f\"RAM usage when loading a {stack_smol.dataset_size / (1024**3):.3f}GB dataset: {(mem_after - mem_before)} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4120c82d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Apache Arrow Backend ‚û°Ô∏è Fast Iteration\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b135629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.97 s, sys: 318 ms, total: 3.29 s\n",
      "Wall time: 3.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 1000\n",
    "for i in range(0, len(stack_smol), batch_size):\n",
    "    batch = stack_smol[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e963d633",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 style=\"text-align:center;\">Loading your own files</h2>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696eb2e2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<table><thead><tr><th align=\"center\">Data format</th> <th align=\"center\">Loading script</th> <th align=\"center\">Example</th></tr></thead> <tbody><tr><td align=\"center\">CSV &amp; TSV</td> <td align=\"center\"><code>csv</code></td> <td align=\"center\"><code>load_dataset(\"csv\", data_files=\"my_file.csv\")</code></td></tr> <tr><td align=\"center\">Text files</td> <td align=\"center\"><code>text</code></td> <td align=\"center\"><code>load_dataset(\"text\", data_files=\"my_file.txt\")</code></td></tr> <tr><td align=\"center\">JSON &amp; JSON Lines</td> <td align=\"center\"><code>json</code></td> <td align=\"center\"><code>load_dataset(\"json\", data_files=\"my_file.jsonl\")</code></td></tr> <tr><td align=\"center\">Pickled DataFrames</td> <td align=\"center\"><code>pandas</code></td> <td align=\"center\"><code>load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")</code></td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c26b2-a8e6-4170-98a4-e1b5705b2fc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"text-align:center;\">ü§ó Tokenizers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c4374-e15e-4821-8cd9-5f72f999300e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src=\"images/tokenization_pipeline.svg\" width=1200></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7464f782",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentence = \"Hello how are U tday?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d69a31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"text-align:center;\">Classifiying Text</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab0ad3",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><h3>Transformers as Feature Extractors</h3></li>\n",
    "    <br>\n",
    "    <li><h3>Fine-tuning Transformers</h3></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02bb29f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"text-align:center;\">More Ways of Classifiying Text</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97cac6d",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><h3>Sentence Transformers as Feature Extractors</h3></li>\n",
    "    <br>\n",
    "    <li><h3>Pattern-Exploiting Training (PET, ADAPET)</h3></li>\n",
    "    <br>\n",
    "    <li><h3>Sentence Transformer Fine-tuning (SetFit)</h3></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a827b91-74d1-407d-8124-e4e500f8a684",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"text-align:center;\">(Re)sources</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5d966",
   "metadata": {},
   "source": [
    "- https://github.com/nlp-with-transformers/notebooks\n",
    "\n",
    "- https://huggingface.co/docs\n",
    "\n",
    "- https://github.com/huggingface/course / https://github.com/huggingface/notebooks\n",
    "\n",
    "- https://github.com/NielsRogge/Transformers-Tutorials\n",
    "\n",
    "<center><a href=\"https://www.oreilly.com/library/view/natural-language-processing/9781098103231/\"><img src=\"images/book_cover.png\" width=400></a></center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
